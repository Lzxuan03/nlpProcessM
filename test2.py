import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import warnings
import time
from tqdm import tqdm

warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

# è®¾å¤‡é…ç½® - å¼ºåˆ¶ä½¿ç”¨CPU
device = torch.device('cpu')
print(f"Using device: {device}")


class SecurityDataset(Dataset):
    def __init__(self, texts, structured_features, labels, tokenizer, max_len=128):
        self.texts = texts
        self.structured_features = structured_features
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        structured_feature = self.structured_features[idx]
        label = self.labels[idx]

        # å¤„ç†æ–‡æœ¬æ•°æ®
        encoding = self.tokenizer.encode_plus(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'structured_features': torch.FloatTensor(structured_feature),
            'labels': torch.tensor(label, dtype=torch.long)
        }


class HybridLSTMClassifier(nn.Module):
    def __init__(self, bert_model_name, structured_feature_dim, num_classes, hidden_dim=128,
                 lstm_layers=1, dropout_rate=0.3):
        super(HybridLSTMClassifier, self).__init__()

        # BERTæ–‡æœ¬ç¼–ç å™¨
        self.bert = BertModel.from_pretrained(bert_model_name)
        bert_hidden_size = self.bert.config.hidden_size

        # å†»ç»“BERTçš„å¤§éƒ¨åˆ†å±‚ä»¥å‡å°‘è®¡ç®—é‡
        for param in list(self.bert.parameters())[:-20]:  # åªè®­ç»ƒæœ€åå‡ å±‚
            param.requires_grad = False

        # æ–‡æœ¬LSTMç¼–ç å™¨
        self.text_lstm = nn.LSTM(
            input_size=bert_hidden_size,
            hidden_size=hidden_dim,
            num_layers=lstm_layers,
            batch_first=True,
            bidirectional=True,
            dropout=dropout_rate if lstm_layers > 1 else 0
        )

        # ç»“æ„åŒ–ç‰¹å¾å¤„ç†ï¼ˆä½¿ç”¨ç®€å•çš„å…¨è¿æ¥å±‚è€Œä¸æ˜¯LSTMï¼‰
        self.struct_fc = nn.Sequential(
            nn.Linear(structured_feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim // 2)
        )

        # æ³¨æ„åŠ›æœºåˆ¶
        self.text_attention = nn.Sequential(
            nn.Linear(hidden_dim * 2, hidden_dim),
            nn.Tanh(),
            nn.Linear(hidden_dim, 1),
            nn.Dropout(dropout_rate)
        )

        # åˆ†ç±»å™¨
        combined_features_size = hidden_dim * 2 + hidden_dim // 2  # æ–‡æœ¬LSTMè¾“å‡º + ç»“æ„åŒ–ç‰¹å¾è¾“å‡º

        self.classifier = nn.Sequential(
            nn.Linear(combined_features_size, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim // 2, num_classes)
        )

        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, input_ids, attention_mask, structured_features):
        # BERTç¼–ç 
        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        text_features = bert_output.last_hidden_state

        # æ–‡æœ¬LSTMå¤„ç†
        text_lstm_out, (text_hidden, _) = self.text_lstm(text_features)

        # æ–‡æœ¬æ³¨æ„åŠ›æœºåˆ¶
        attention_weights = torch.softmax(self.text_attention(text_lstm_out), dim=1)
        text_attended = torch.sum(attention_weights * text_lstm_out, dim=1)

        # ç»“æ„åŒ–ç‰¹å¾å¤„ç†
        struct_features = self.struct_fc(structured_features)

        # ç‰¹å¾èåˆ
        combined_features = torch.cat([text_attended, struct_features], dim=1)
        combined_features = self.dropout(combined_features)

        # åˆ†ç±»
        logits = self.classifier(combined_features)

        return logits


class SecurityAttackClassifier:
    def __init__(self, bert_model_name='bert-base-uncased', max_len=128):
        self.bert_model_name = bert_model_name
        self.max_len = max_len
        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)
        self.label_encoder = LabelEncoder()
        self.scaler = StandardScaler()
        self.device = device

    def load_and_preprocess_data(self, file_path='cybersecurity_attacks.csv'):
        """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®"""
        print("åŠ è½½æ•°æ®...")
        df = pd.read_csv(file_path)
        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")

        # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
        print(f"æ•°æ®åˆ—å: {df.columns.tolist()}")
        if 'Attack Type' in df.columns:
            print(f"æ”»å‡»ç±»å‹åˆ†å¸ƒ:\n{df['Attack Type'].value_counts()}")
        print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:\n{df.isnull().sum()}")

        # æ£€æŸ¥Payload Dataå­—æ®µ
        if 'Payload Data' not in df.columns:
            print("è­¦å‘Š: æ•°æ®é›†ä¸­æ²¡æœ‰æ‰¾åˆ°'Payload Data'å­—æ®µï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡æœ¬å­—æ®µ")
            # å°è¯•æ‰¾åˆ°æ–‡æœ¬å­—æ®µ
            text_columns = [col for col in df.columns if df[col].dtype == 'object' and col != 'Attack Type']
            if text_columns:
                text_column = text_columns[0]
                print(f"ä½¿ç”¨ '{text_column}' ä½œä¸ºæ–‡æœ¬å­—æ®µ")
            else:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ–‡æœ¬å­—æ®µ")
        else:
            text_column = 'Payload Data'

        return self.preprocess_data(df, text_column=text_column)

    def preprocess_data(self, df, text_column='Payload Data', label_column='Attack Type'):
        """é¢„å¤„ç†æ•°æ®"""
        print("é¢„å¤„ç†æ•°æ®...")

        # å¤„ç†æ ‡ç­¾
        if label_column not in df.columns:
            # å¦‚æœæ²¡æœ‰æ ‡ç­¾åˆ—ï¼Œåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿæ ‡ç­¾
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾åˆ—ï¼Œåˆ›å»ºè™šæ‹Ÿæ ‡ç­¾")
            labels = np.zeros(len(df))
            self.label_encoder.fit(['Unknown'])
        else:
            labels = self.label_encoder.fit_transform(df[label_column])
        print(f"æ ‡ç­¾ç±»åˆ«: {self.label_encoder.classes_}")

        # é€‰æ‹©ç»“æ„åŒ–ç‰¹å¾ - åŸºäºé¢˜ç›®æè¿°çš„é‡è¦ç‰¹å¾
        # æ•°å€¼ç‰¹å¾
        numeric_features = []
        potential_numeric = ['Packet Length', 'Anomaly Score', 'Severity Level', 'packet_length',
                             'anomaly_score', 'severity_level', 'Duration', 'duration',
                             'Source Port', 'source_port', 'Destination Port', 'destination_port']

        for feature in potential_numeric:
            if feature in df.columns:
                # å°è¯•è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
                try:
                    df[feature] = pd.to_numeric(df[feature], errors='coerce')
                    numeric_features.append(feature)
                except:
                    print(f"è­¦å‘Š: æ— æ³•å°†ç‰¹å¾ {feature} è½¬æ¢ä¸ºæ•°å€¼ç±»å‹ï¼Œè·³è¿‡")

        # ç±»åˆ«ç‰¹å¾
        categorical_features = []
        potential_categorical = ['Protocol', 'Flow Type', 'Action Taken', 'protocol',
                                 'flow_type', 'action_taken', 'Source IP', 'source_ip',
                                 'Destination IP', 'destination_ip']

        for feature in potential_categorical:
            if feature in df.columns and feature not in numeric_features:
                categorical_features.append(feature)

        print(f"ä½¿ç”¨çš„æ•°å€¼ç‰¹å¾: {numeric_features}")
        print(f"ä½¿ç”¨çš„ç±»åˆ«ç‰¹å¾: {categorical_features}")

        # å¤„ç†æ•°å€¼ç‰¹å¾
        if numeric_features:
            # å¡«å……ç¼ºå¤±å€¼
            numeric_data = df[numeric_features].fillna(0).values
            # æ ‡å‡†åŒ–æ•°å€¼ç‰¹å¾
            if len(numeric_data) > 0:
                numeric_data = self.scaler.fit_transform(numeric_data)
        else:
            numeric_data = np.zeros((len(df), 0))

        # å¤„ç†ç±»åˆ«ç‰¹å¾
        if categorical_features:
            # å¡«å……ç¼ºå¤±å€¼
            categorical_df = df[categorical_features].fillna('unknown')
            # å¯¹ç±»åˆ«ç‰¹å¾è¿›è¡Œone-hotç¼–ç 
            categorical_data = pd.get_dummies(categorical_df).values
        else:
            categorical_data = np.zeros((len(df), 0))

        # åˆå¹¶ç»“æ„åŒ–ç‰¹å¾
        structured_features = np.hstack([numeric_data, categorical_data])

        print(f"ç»“æ„åŒ–ç‰¹å¾ç»´åº¦: {structured_features.shape}")
        print(f"æ–‡æœ¬ç‰¹å¾æ•°é‡: {len(df[text_column])}")

        return df[text_column].values, structured_features, labels

    def create_data_loaders(self, texts, structured_features, labels, batch_size=8):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        # åˆ’åˆ†æ•°æ®é›† 7:1:2
        X_text_temp, X_text_test, X_struct_temp, X_struct_test, y_temp, y_test = train_test_split(
            texts, structured_features, labels, test_size=0.2, random_state=42, stratify=labels
        )

        X_text_train, X_text_val, X_struct_train, X_struct_val, y_train, y_val = train_test_split(
            X_text_temp, X_struct_temp, y_temp, test_size=0.125, random_state=42, stratify=y_temp  # 0.125 * 0.8 = 0.1
        )

        print(f"è®­ç»ƒé›†å¤§å°: {len(X_text_train)}")
        print(f"éªŒè¯é›†å¤§å°: {len(X_text_val)}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(X_text_test)}")

        # åˆ›å»ºæ•°æ®é›†
        train_dataset = SecurityDataset(X_text_train, X_struct_train, y_train, self.tokenizer, self.max_len)
        val_dataset = SecurityDataset(X_text_val, X_struct_val, y_val, self.tokenizer, self.max_len)
        test_dataset = SecurityDataset(X_text_test, X_struct_test, y_test, self.tokenizer, self.max_len)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        return train_loader, val_loader, test_loader

    def train(self, train_loader, val_loader, num_epochs=5, learning_rate=2e-5):
        """è®­ç»ƒæ¨¡å‹"""
        structured_feature_dim = train_loader.dataset[0]['structured_features'].shape[0]
        num_classes = len(self.label_encoder.classes_)

        print(f"ç»“æ„åŒ–ç‰¹å¾ç»´åº¦: {structured_feature_dim}")
        print(f"åˆ†ç±»ç±»åˆ«æ•°: {num_classes}")

        self.model = HybridLSTMClassifier(
            self.bert_model_name,
            structured_feature_dim,
            num_classes,
            hidden_dim=128,  # å‡å°éšè—å±‚ç»´åº¦
            lstm_layers=1,  # å‡å°‘LSTMå±‚æ•°
            dropout_rate=0.3
        ).to(self.device)

        # ä¼˜åŒ–å™¨
        optimizer = AdamW([
            {'params': [p for p in self.model.bert.parameters() if p.requires_grad], 'lr': learning_rate / 10},
            {'params': self.model.text_lstm.parameters(), 'lr': learning_rate},
            {'params': self.model.struct_fc.parameters(), 'lr': learning_rate},
            {'params': self.model.classifier.parameters(), 'lr': learning_rate}
        ], weight_decay=0.01)

        # æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss()

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        total_steps = len(train_loader) * num_epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=0,
            num_training_steps=total_steps
        )

        best_accuracy = 0
        train_losses = []
        val_accuracies = []

        print("\nå¼€å§‹è®­ç»ƒ...")
        print("=" * 60)

        for epoch in range(num_epochs):
            start_time = time.time()

            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            total_loss = 0
            batch_count = 0

            # ä½¿ç”¨tqdmæ˜¾ç¤ºè®­ç»ƒè¿›åº¦
            train_pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} [è®­ç»ƒ]')

            for batch in train_pbar:
                optimizer.zero_grad()

                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                structured_features = batch['structured_features'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(input_ids, attention_mask, structured_features)
                loss = criterion(outputs, labels)

                loss.backward()
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                optimizer.step()
                scheduler.step()

                total_loss += loss.item()
                batch_count += 1

                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰æŸå¤±
                train_pbar.set_postfix({
                    'æŸå¤±': f'{loss.item():.4f}',
                    'å¹³å‡æŸå¤±': f'{total_loss / batch_count:.4f}'
                })

            avg_loss = total_loss / len(train_loader)
            train_losses.append(avg_loss)

            # éªŒè¯é˜¶æ®µ
            val_accuracy = self.evaluate(val_loader)
            val_accuracies.append(val_accuracy)

            epoch_time = time.time() - start_time

            print(f"\nEpoch {epoch + 1}/{num_epochs} å®Œæˆ")
            print(f"è®­ç»ƒè€—æ—¶: {epoch_time:.2f}ç§’")
            print(f'è®­ç»ƒæŸå¤±: {avg_loss:.4f}')
            print(f'éªŒè¯é›†å‡†ç¡®ç‡: {val_accuracy:.4f}')

            # æ˜¾ç¤ºå½“å‰æœ€ä½³å‡†ç¡®ç‡
            if val_accuracy > best_accuracy:
                best_accuracy = val_accuracy
                print(f"ğŸ‰ æ–°çš„æœ€ä½³å‡†ç¡®ç‡! ä¿å­˜æ¨¡å‹...")
            else:
                print(f"å½“å‰æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")

            print('-' * 50)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_accuracy > best_accuracy:
                best_accuracy = val_accuracy
                torch.save(self.model.state_dict(), 'best_model.pth')

        # åŠ è½½æœ€ä½³æ¨¡å‹
        self.model.load_state_dict(torch.load('best_model.pth', map_location=device))

        print(f"\nè®­ç»ƒå®Œæˆ!")
        print(f"æœ€ç»ˆéªŒè¯é›†æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")

        return train_losses, val_accuracies

    def evaluate(self, data_loader):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        predictions = []
        true_labels = []

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¯„ä¼°è¿›åº¦
        eval_pbar = tqdm(data_loader, desc='[éªŒè¯]')

        with torch.no_grad():
            for batch in eval_pbar:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                structured_features = batch['structured_features'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(input_ids, attention_mask, structured_features)
                _, preds = torch.max(outputs, dim=1)

                predictions.extend(preds.cpu().tolist())
                true_labels.extend(labels.cpu().tolist())

                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡å‡†ç¡®ç‡
                batch_acc = accuracy_score(labels.cpu().tolist(), preds.cpu().tolist())
                eval_pbar.set_postfix({'æ‰¹æ¬¡å‡†ç¡®ç‡': f'{batch_acc:.4f}'})

        return accuracy_score(true_labels, predictions)

    def predict(self, data_loader):
        """é¢„æµ‹"""
        self.model.eval()
        predictions = []
        true_labels = []

        # ä½¿ç”¨tqdmæ˜¾ç¤ºé¢„æµ‹è¿›åº¦
        pred_pbar = tqdm(data_loader, desc='[é¢„æµ‹]')

        with torch.no_grad():
            for batch in pred_pbar:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                structured_features = batch['structured_features'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(input_ids, attention_mask, structured_features)
                _, preds = torch.max(outputs, dim=1)

                predictions.extend(preds.cpu().tolist())
                true_labels.extend(labels.cpu().tolist())

        return predictions, true_labels


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–åˆ†ç±»å™¨
    classifier = SecurityAttackClassifier()

    try:
        # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
        texts, structured_features, labels = classifier.load_and_preprocess_data('cybersecurity_attacks.csv')

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = classifier.create_data_loaders(
            texts, structured_features, labels, batch_size=8  # å‡å°æ‰¹å¤§å°
        )

        # è®­ç»ƒæ¨¡å‹
        print("\nå¼€å§‹è®­ç»ƒæ¨¡å‹...")
        train_losses, val_accuracies = classifier.train(
            train_loader, val_loader, num_epochs=5, learning_rate=2e-5  # å‡å°‘è®­ç»ƒè½®æ•°
        )

        # æ˜¾ç¤ºè®­ç»ƒå†å²
        print("\nè®­ç»ƒå†å²:")
        for epoch, (loss, acc) in enumerate(zip(train_losses, val_accuracies)):
            print(f"Epoch {epoch + 1}: æŸå¤±={loss:.4f}, éªŒè¯å‡†ç¡®ç‡={acc:.4f}")

        # æµ‹è¯•æ¨¡å‹
        print("\næµ‹è¯•æ¨¡å‹...")
        test_predictions, test_true = classifier.predict(test_loader)
        test_accuracy = accuracy_score(test_true, test_predictions)

        print(f"\næœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
        print(f"\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(test_true, test_predictions,
                                    target_names=classifier.label_encoder.classes_))

        # æ˜¾ç¤ºæ··æ·†çŸ©é˜µ
        cm = confusion_matrix(test_true, test_predictions)
        print(f"\næ··æ·†çŸ©é˜µ:")
        print(cm)

    except FileNotFoundError:
        print("é”™è¯¯: æœªæ‰¾åˆ° 'cybersecurity_attacks.csv' æ–‡ä»¶")
        print("è¯·ç¡®ä¿æ–‡ä»¶å­˜åœ¨äºå½“å‰ç›®å½•ä¸­")

        # åˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•
        print("\nåˆ›å»ºç¤ºä¾‹æ•°æ®ç”¨äºæµ‹è¯•...")
        create_sample_data()

        # é‡æ–°è¿è¡Œ
        print("\nä½¿ç”¨ç¤ºä¾‹æ•°æ®é‡æ–°è¿è¡Œ...")
        texts, structured_features, labels = classifier.load_and_preprocess_data('cybersecurity_attacks.csv')

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = classifier.create_data_loaders(
            texts, structured_features, labels, batch_size=8
        )

        # è®­ç»ƒæ¨¡å‹
        print("\nå¼€å§‹è®­ç»ƒæ¨¡å‹...")
        train_losses, val_accuracies = classifier.train(
            train_loader, val_loader, num_epochs=3, learning_rate=2e-5
        )

        # æ˜¾ç¤ºè®­ç»ƒå†å²
        print("\nè®­ç»ƒå†å²:")
        for epoch, (loss, acc) in enumerate(zip(train_losses, val_accuracies)):
            print(f"Epoch {epoch + 1}: æŸå¤±={loss:.4f}, éªŒè¯å‡†ç¡®ç‡={acc:.4f}")

        # æµ‹è¯•æ¨¡å‹
        print("\næµ‹è¯•æ¨¡å‹...")
        test_predictions, test_true = classifier.predict(test_loader)
        test_accuracy = accuracy_score(test_true, test_predictions)

        print(f"\næœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")

    except Exception as e:
        print(f"å¤„ç†æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹ç½‘ç»œå®‰å…¨æ•°æ®"""
    np.random.seed(42)

    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    n_samples = 1000

    data = {
        'Payload Data': [
            f"GET /malicious.php?cmd={np.random.choice(['exec', 'download', 'inject'])} HTTP/1.1"
            for _ in range(n_samples)
        ],
        'Packet Length': np.random.randint(50, 1500, n_samples),
        'Protocol': np.random.choice(['TCP', 'UDP', 'HTTP', 'HTTPS'], n_samples),
        'Source Port': np.random.randint(1024, 65535, n_samples),
        'Destination Port': np.random.choice([80, 443, 22, 21, 53], n_samples),
        'Anomaly Score': np.random.uniform(0, 1, n_samples),
        'Severity Level': np.random.choice([1, 2, 3, 4], n_samples),  # ä½¿ç”¨æ•°å€¼è€Œä¸æ˜¯å­—ç¬¦ä¸²
        'Flow Type': np.random.choice(['Normal', 'Suspicious', 'Malicious'], n_samples),
        'Action Taken': np.random.choice(['Allow', 'Block', 'Monitor'], n_samples),
        'Attack Type': np.random.choice(
            ['DDoS', 'Malware', 'Phishing', 'Brute Force', 'SQL Injection', 'Normal'],
            n_samples,
            p=[0.15, 0.15, 0.15, 0.15, 0.15, 0.25]
        )
    }

    df = pd.DataFrame(data)
    df.to_csv('cybersecurity_attacks.csv', index=False)
    print("å·²åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶ 'cybersecurity_attacks.csv'")


if __name__ == "__main__":
    main()