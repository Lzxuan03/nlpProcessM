import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import DistilBertTokenizer, DistilBertModel
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import warnings
import time
from tqdm import tqdm

warnings.filterwarnings('ignore')

# è®¾ç½®éšæœºç§å­
torch.manual_seed(42)
np.random.seed(42)

# è®¾å¤‡é…ç½® - å¼ºåˆ¶ä½¿ç”¨CPU
device = torch.device('cpu')
print(f"Using device: {device}")


class SecurityDataset(Dataset):
    def __init__(self, texts, structured_features, labels, tokenizer, max_len=64):
        self.texts = texts
        self.structured_features = structured_features
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        structured_feature = self.structured_features[idx]
        label = self.labels[idx]

        # å¤„ç†æ–‡æœ¬æ•°æ®
        encoding = self.tokenizer(
            text,
            add_special_tokens=True,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_attention_mask=True,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'structured_features': torch.FloatTensor(structured_feature),
            'labels': torch.tensor(label, dtype=torch.long)
        }


class FastSecurityClassifier(nn.Module):
    def __init__(self, model_name, structured_feature_dim, num_classes, hidden_dim=64, dropout_rate=0.2):
        super(FastSecurityClassifier, self).__init__()

        # ä½¿ç”¨DistilBERTæ–‡æœ¬ç¼–ç å™¨
        self.distilbert = DistilBertModel.from_pretrained(model_name)
        bert_hidden_size = self.distilbert.config.hidden_size

        # å†»ç»“æ‰€æœ‰DistilBERTå‚æ•°ï¼Œåªä½œä¸ºç‰¹å¾æå–å™¨
        for param in self.distilbert.parameters():
            param.requires_grad = False

        # ç®€åŒ–æ–‡æœ¬ç‰¹å¾å¤„ç†
        self.text_projection = nn.Sequential(
            nn.Linear(bert_hidden_size, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )

        # ç®€åŒ–ç»“æ„åŒ–ç‰¹å¾å¤„ç†
        self.struct_projection = nn.Sequential(
            nn.Linear(structured_feature_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate)
        )

        # ç®€åŒ–åˆ†ç±»å™¨
        combined_features_size = hidden_dim + hidden_dim // 2
        self.classifier = nn.Sequential(
            nn.Linear(combined_features_size, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, num_classes)
        )

    def forward(self, input_ids, attention_mask, structured_features):
        # DistilBERTç¼–ç 
        with torch.no_grad():  # ä¸è®¡ç®—æ¢¯åº¦
            distilbert_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)
            # ä½¿ç”¨[CLS] tokenä½œä¸ºåºåˆ—è¡¨ç¤º
            text_features = distilbert_output.last_hidden_state[:, 0, :]

        # æ–‡æœ¬ç‰¹å¾æŠ•å½±
        text_proj = self.text_projection(text_features)

        # ç»“æ„åŒ–ç‰¹å¾æŠ•å½±
        struct_proj = self.struct_projection(structured_features)

        # ç‰¹å¾èåˆå’Œåˆ†ç±»
        combined_features = torch.cat([text_proj, struct_proj], dim=1)
        logits = self.classifier(combined_features)

        return logits


class FastSecurityAttackClassifier:
    def __init__(self, model_name='distilbert-base-uncased', max_len=64):
        self.model_name = model_name
        self.max_len = max_len
        self.tokenizer = DistilBertTokenizer.from_pretrained(model_name)
        self.label_encoder = LabelEncoder()
        self.scaler = StandardScaler()
        self.device = device

    def load_and_preprocess_data(self, file_path='cybersecurity_attacks.csv'):
        """åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®"""
        print("åŠ è½½æ•°æ®...")
        df = pd.read_csv(file_path)
        print(f"æ•°æ®å½¢çŠ¶: {df.shape}")

        # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
        print(f"æ•°æ®åˆ—å: {df.columns.tolist()}")
        if 'Attack Type' in df.columns:
            print(f"æ”»å‡»ç±»å‹åˆ†å¸ƒ:\n{df['Attack Type'].value_counts()}")
        print(f"ç¼ºå¤±å€¼ç»Ÿè®¡:\n{df.isnull().sum()}")

        # æ£€æŸ¥Payload Dataå­—æ®µ
        if 'Payload Data' not in df.columns:
            print("è­¦å‘Š: æ•°æ®é›†ä¸­æ²¡æœ‰æ‰¾åˆ°'Payload Data'å­—æ®µï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªæ–‡æœ¬å­—æ®µ")
            text_columns = [col for col in df.columns if df[col].dtype == 'object' and col != 'Attack Type']
            if text_columns:
                text_column = text_columns[0]
                print(f"ä½¿ç”¨ '{text_column}' ä½œä¸ºæ–‡æœ¬å­—æ®µ")
            else:
                raise ValueError("æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ–‡æœ¬å­—æ®µ")
        else:
            text_column = 'Payload Data'

        return self.preprocess_data(df, text_column=text_column)

    def preprocess_data(self, df, text_column='Payload Data', label_column='Attack Type'):
        """é¢„å¤„ç†æ•°æ®"""
        print("é¢„å¤„ç†æ•°æ®...")

        # å¤„ç†æ ‡ç­¾
        if label_column not in df.columns:
            print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°æ ‡ç­¾åˆ—ï¼Œåˆ›å»ºè™šæ‹Ÿæ ‡ç­¾")
            labels = np.zeros(len(df))
            self.label_encoder.fit(['Unknown'])
        else:
            labels = self.label_encoder.fit_transform(df[label_column])
        print(f"æ ‡ç­¾ç±»åˆ«: {self.label_encoder.classes_}")

        # é€‰æ‹©æœ€é‡è¦çš„ç‰¹å¾ä»¥å‡å°‘ç»´åº¦
        important_features = []

        # æ•°å€¼ç‰¹å¾
        numeric_candidates = ['Packet Length', 'packet_length', 'Anomaly Score', 'anomaly_score',
                              'Severity Level', 'severity_level', 'Source Port', 'source_port']

        # ç±»åˆ«ç‰¹å¾
        categorical_candidates = ['Protocol', 'protocol', 'Flow Type', 'flow_type']

        for feature in numeric_candidates:
            if feature in df.columns:
                try:
                    df[feature] = pd.to_numeric(df[feature], errors='coerce')
                    important_features.append(feature)
                    if len(important_features) >= 3:  # åªå–å‰3ä¸ªæ•°å€¼ç‰¹å¾
                        break
                except:
                    continue

        for feature in categorical_candidates:
            if feature in df.columns and feature not in important_features:
                important_features.append(feature)
                if len(important_features) >= 5:  # æ€»å…±ä¸è¶…è¿‡5ä¸ªç‰¹å¾
                    break

        print(f"ä½¿ç”¨çš„é‡è¦ç‰¹å¾: {important_features}")

        # å¤„ç†æ•°å€¼ç‰¹å¾
        numeric_features = [f for f in important_features if df[f].dtype in ['int64', 'float64']]
        categorical_features = [f for f in important_features if f not in numeric_features]

        # å¤„ç†æ•°å€¼ç‰¹å¾
        if numeric_features:
            numeric_data = df[numeric_features].fillna(0).values
            if len(numeric_data) > 0:
                numeric_data = self.scaler.fit_transform(numeric_data)
        else:
            numeric_data = np.zeros((len(df), 0))

        # å¤„ç†ç±»åˆ«ç‰¹å¾
        if categorical_features:
            categorical_df = df[categorical_features].fillna('unknown')
            categorical_data = pd.get_dummies(categorical_df).values
        else:
            categorical_data = np.zeros((len(df), 0))

        # åˆå¹¶ç»“æ„åŒ–ç‰¹å¾
        structured_features = np.hstack([numeric_data, categorical_data])

        print(f"ç»“æ„åŒ–ç‰¹å¾ç»´åº¦: {structured_features.shape}")
        print(f"æ–‡æœ¬ç‰¹å¾æ•°é‡: {len(df[text_column])}")

        return df[text_column].values, structured_features, labels

    def create_data_loaders(self, texts, structured_features, labels, batch_size=16):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        # åˆ’åˆ†æ•°æ®é›† 8:1:1 - å‡å°‘éªŒè¯é›†æ¯”ä¾‹
        X_text_temp, X_text_test, X_struct_temp, X_struct_test, y_temp, y_test = train_test_split(
            texts, structured_features, labels, test_size=0.1, random_state=42, stratify=labels
        )

        X_text_train, X_text_val, X_struct_train, X_struct_val, y_train, y_val = train_test_split(
            X_text_temp, X_struct_temp, y_temp, test_size=0.1, random_state=42, stratify=y_temp
        )

        print(f"è®­ç»ƒé›†å¤§å°: {len(X_text_train)}")
        print(f"éªŒè¯é›†å¤§å°: {len(X_text_val)}")
        print(f"æµ‹è¯•é›†å¤§å°: {len(X_text_test)}")

        # åˆ›å»ºæ•°æ®é›†
        train_dataset = SecurityDataset(X_text_train, X_struct_train, y_train, self.tokenizer, self.max_len)
        val_dataset = SecurityDataset(X_text_val, X_struct_val, y_val, self.tokenizer, self.max_len)
        test_dataset = SecurityDataset(X_text_test, X_struct_test, y_test, self.tokenizer, self.max_len)

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)
        test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

        return train_loader, val_loader, test_loader

    def train(self, train_loader, val_loader, num_epochs=3, learning_rate=1e-3):
        """è®­ç»ƒæ¨¡å‹"""
        structured_feature_dim = train_loader.dataset[0]['structured_features'].shape[0]
        num_classes = len(self.label_encoder.classes_)

        print(f"ç»“æ„åŒ–ç‰¹å¾ç»´åº¦: {structured_feature_dim}")
        print(f"åˆ†ç±»ç±»åˆ«æ•°: {num_classes}")

        self.model = FastSecurityClassifier(
            self.model_name,
            structured_feature_dim,
            num_classes,
            hidden_dim=64,
            dropout_rate=0.2
        ).to(self.device)

        # ä¼˜åŒ–å™¨ - åªä¼˜åŒ–æŠ•å½±å±‚å’Œåˆ†ç±»å™¨
        optimizer = AdamW(
            [p for p in self.model.parameters() if p.requires_grad],
            lr=learning_rate,
            weight_decay=0.01
        )

        # æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss()

        best_accuracy = 0
        train_losses = []
        val_accuracies = []

        print("\nå¼€å§‹è®­ç»ƒ...")
        print("=" * 60)

        for epoch in range(num_epochs):
            start_time = time.time()

            # è®­ç»ƒé˜¶æ®µ
            self.model.train()
            total_loss = 0
            batch_count = 0

            # ä½¿ç”¨tqdmæ˜¾ç¤ºè®­ç»ƒè¿›åº¦æ¡
            train_pbar = tqdm(train_loader, desc=f'Epoch {epoch + 1}/{num_epochs} [è®­ç»ƒ]', leave=False)

            for batch in train_pbar:
                optimizer.zero_grad()

                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                structured_features = batch['structured_features'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(input_ids, attention_mask, structured_features)
                loss = criterion(outputs, labels)

                loss.backward()
                optimizer.step()

                total_loss += loss.item()
                batch_count += 1

                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰æŸå¤±
                train_pbar.set_postfix({
                    'æŸå¤±': f'{loss.item():.4f}',
                    'å¹³å‡æŸå¤±': f'{total_loss / batch_count:.4f}'
                })

            avg_loss = total_loss / len(train_loader)
            train_losses.append(avg_loss)

            # éªŒè¯é˜¶æ®µ
            val_accuracy = self.evaluate(val_loader)
            val_accuracies.append(val_accuracy)

            epoch_time = time.time() - start_time

            print(f"\nEpoch {epoch + 1}/{num_epochs} å®Œæˆ")
            print(f"è®­ç»ƒè€—æ—¶: {epoch_time:.2f}ç§’")
            print(f'è®­ç»ƒæŸå¤±: {avg_loss:.4f}')
            print(f'éªŒè¯é›†å‡†ç¡®ç‡: {val_accuracy:.4f}')

            # æ˜¾ç¤ºå½“å‰æœ€ä½³å‡†ç¡®ç‡
            if val_accuracy > best_accuracy:
                best_accuracy = val_accuracy
                print(f"ğŸ‰ æ–°çš„æœ€ä½³å‡†ç¡®ç‡! ä¿å­˜æ¨¡å‹...")
            else:
                print(f"å½“å‰æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")

            print('-' * 50)

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if val_accuracy > best_accuracy:
                best_accuracy = val_accuracy
                torch.save(self.model.state_dict(), 'best_model.pth')

        # åŠ è½½æœ€ä½³æ¨¡å‹
        self.model.load_state_dict(torch.load('best_model.pth', map_location=device))

        print(f"\nè®­ç»ƒå®Œæˆ!")
        print(f"æœ€ç»ˆéªŒè¯é›†æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")

        return train_losses, val_accuracies

    def evaluate(self, data_loader):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        predictions = []
        true_labels = []

        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¯„ä¼°è¿›åº¦æ¡
        eval_pbar = tqdm(data_loader, desc='[éªŒè¯]', leave=False)

        with torch.no_grad():
            for batch in eval_pbar:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                structured_features = batch['structured_features'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(input_ids, attention_mask, structured_features)
                _, preds = torch.max(outputs, dim=1)

                predictions.extend(preds.cpu().tolist())
                true_labels.extend(labels.cpu().tolist())

                # æ›´æ–°è¿›åº¦æ¡æ˜¾ç¤ºå½“å‰æ‰¹æ¬¡å‡†ç¡®ç‡
                batch_acc = accuracy_score(labels.cpu().tolist(), preds.cpu().tolist())
                eval_pbar.set_postfix({'æ‰¹æ¬¡å‡†ç¡®ç‡': f'{batch_acc:.4f}'})

        return accuracy_score(true_labels, predictions)

    def predict(self, data_loader):
        """é¢„æµ‹"""
        self.model.eval()
        predictions = []
        true_labels = []

        # ä½¿ç”¨tqdmæ˜¾ç¤ºé¢„æµ‹è¿›åº¦æ¡
        pred_pbar = tqdm(data_loader, desc='[æµ‹è¯•]', leave=False)

        with torch.no_grad():
            for batch in pred_pbar:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                structured_features = batch['structured_features'].to(self.device)
                labels = batch['labels'].to(self.device)

                outputs = self.model(input_ids, attention_mask, structured_features)
                _, preds = torch.max(outputs, dim=1)

                predictions.extend(preds.cpu().tolist())
                true_labels.extend(labels.cpu().tolist())

        return predictions, true_labels


def main():
    """ä¸»å‡½æ•°"""
    # åˆå§‹åŒ–åˆ†ç±»å™¨ - ä½¿ç”¨DistilBERT
    classifier = FastSecurityAttackClassifier(model_name='distilbert-base-uncased', max_len=64)

    try:
        # åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
        texts, structured_features, labels = classifier.load_and_preprocess_data('cybersecurity_attacks.csv')

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_loader, val_loader, test_loader = classifier.create_data_loaders(
            texts, structured_features, labels, batch_size=16
        )

        # è®­ç»ƒæ¨¡å‹
        print("\nå¼€å§‹è®­ç»ƒæ¨¡å‹...")
        train_losses, val_accuracies = classifier.train(
            train_loader, val_loader, num_epochs=3, learning_rate=1e-3
        )

        # æ˜¾ç¤ºè®­ç»ƒå†å²
        print("\nè®­ç»ƒå†å²:")
        for epoch, (loss, acc) in enumerate(zip(train_losses, val_accuracies)):
            print(f"Epoch {epoch + 1}: æŸå¤±={loss:.4f}, éªŒè¯å‡†ç¡®ç‡={acc:.4f}")

        # æµ‹è¯•æ¨¡å‹
        print("\næµ‹è¯•æ¨¡å‹...")
        test_predictions, test_true = classifier.predict(test_loader)
        test_accuracy = accuracy_score(test_true, test_predictions)

        print(f"\næœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
        print(f"\nåˆ†ç±»æŠ¥å‘Š:")
        print(classification_report(test_true, test_predictions,
                                    target_names=classifier.label_encoder.classes_))

        # æ˜¾ç¤ºæ··æ·†çŸ©é˜µ
        cm = confusion_matrix(test_true, test_predictions)
        print(f"\næ··æ·†çŸ©é˜µ:")
        print(cm)

    except FileNotFoundError:
        print("é”™è¯¯: æœªæ‰¾åˆ° 'cybersecurity_attacks.csv' æ–‡ä»¶")
        print("åˆ›å»ºç¤ºä¾‹æ•°æ®...")
        create_sample_data()

        # é‡æ–°è¿è¡Œ
        texts, structured_features, labels = classifier.load_and_preprocess_data('cybersecurity_attacks.csv')

        train_loader, val_loader, test_loader = classifier.create_data_loaders(
            texts, structured_features, labels, batch_size=16
        )

        print("\nå¼€å§‹è®­ç»ƒæ¨¡å‹...")
        train_losses, val_accuracies = classifier.train(
            train_loader, val_loader, num_epochs=3, learning_rate=1e-3
        )

        # æ˜¾ç¤ºè®­ç»ƒå†å²
        print("\nè®­ç»ƒå†å²:")
        for epoch, (loss, acc) in enumerate(zip(train_losses, val_accuracies)):
            print(f"Epoch {epoch + 1}: æŸå¤±={loss:.4f}, éªŒè¯å‡†ç¡®ç‡={acc:.4f}")

        test_predictions, test_true = classifier.predict(test_loader)
        test_accuracy = accuracy_score(test_true, test_predictions)
        print(f"\næœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")

    except Exception as e:
        print(f"å¤„ç†æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹ç½‘ç»œå®‰å…¨æ•°æ®"""
    np.random.seed(42)

    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    n_samples = 800

    data = {
        'Payload Data': [
            f"GET /malicious.php?cmd={np.random.choice(['exec', 'download', 'inject'])} HTTP/1.1"
            for _ in range(n_samples)
        ],
        'Packet Length': np.random.randint(50, 1500, n_samples),
        'Protocol': np.random.choice(['TCP', 'UDP', 'HTTP'], n_samples),
        'Anomaly Score': np.random.uniform(0, 1, n_samples),
        'Severity Level': np.random.choice([1, 2, 3, 4], n_samples),
        'Flow Type': np.random.choice(['Normal', 'Suspicious', 'Malicious'], n_samples),
        'Attack Type': np.random.choice(
            ['DDoS', 'Malware', 'Phishing', 'Normal'],
            n_samples,
            p=[0.25, 0.25, 0.25, 0.25]
        )
    }

    df = pd.DataFrame(data)
    df.to_csv('cybersecurity_attacks.csv', index=False)
    print("å·²åˆ›å»ºç¤ºä¾‹æ•°æ®æ–‡ä»¶ 'cybersecurity_attacks.csv'")


if __name__ == "__main__":
    main()
